#!/usr/bin/env perl
## Copyright Â© 2012 by Daniel Friesel <derf@finalrewind.org>
## License: WTFPL <http://sam.zoy.org/wtfpl>
use strict;
use warnings;
use 5.010;

use Digest::SHA qw(sha512);
use File::Slurp qw(read_dir read_file write_file);
use List::MoreUtils qw(any);
use LWP::UserAgent;
use XML::LibXML;

my ($base_url) = @ARGV;
my $url = $base_url;

local $| = 1;

my $ua
  = LWP::UserAgent->new( agent =>
'Mozilla/5.0 (X11; Linux x86_64; rv:7.0.1) Gecko/20100101 Firefox/7.0.1 Iceweasel/7.0.1'
  );

my $xp_post = XML::LibXML::XPathExpression->new(
	'//div[@class="post" or @class="photo post" or @class="video post" or @class="post " or @class="post photo" or @class="post video"]'
	. ' | //li[@class="post photo" or @class="post photoset"]');
my $xp_multi = XML::LibXML::XPathExpression->new('.//iframe');
my $xp_media = XML::LibXML::XPathExpression->new('./div[@class="media" or @id="postmiddle"]/a');
my $xp_next  = XML::LibXML::XPathExpression->new('//div[@id="navigation" or @id="index"]/a'
	. ' | //p[@class="previous-next"]/a | //div[@id="pagenav"]/div/a');
my $xp_set
  = XML::LibXML::XPathExpression->new('//div[@class="photoset"]/div/a');

my @hash_cache;
my $text_cache;

sub stash_text {
	($text_cache) = @_;
}

sub is_dup {
	my ($newfile) = @_;
	my $newhash = sha512(read_file($newfile));

	if (@hash_cache) {
		return any { $_ eq $newhash } @hash_cache;
	}

	for my $file (read_dir('.')) {
		if ($file eq $newfile) {
			next;
		}
		my $content = read_file($file);
		my $hash = sha512($content);

		push(@hash_cache, $hash);

		if ($newhash eq $hash) {
			return 1;
		}
	}
	return 0;
}

sub crawl_image {
	my ($url) = @_;
	my ( $no, $id ) = $url =~ m{ ([^/]+) / ([^/]+) $ }ox;
	my $name = "${id}_${no}";

	# we're done here
	if ( -e $name ) {
		say "no new images at $name";
		exit 0;
	}

	say "downloading $name";
	write_file($name, $ua->get($url)->content);

	if (is_dup($name)) {
		unlink($name);
		say "no new images at $name (duplicate)";
		exit 0;
	}

	if ($text_cache) {
		write_file("${id}.txt", {binmode => ':utf8'}, $text_cache);
		$text_cache = undef;
	}
}

sub crawl_subpage {
	my ($url) = @_;
	my $html = XML::LibXML->load_html(
		string            => $ua->get($url)->content,
		recover           => 2,
		suppress_errors   => 1,
		suppress_warnings => 1,
	);

	for my $elem ( $html->findnodes($xp_set) ) {
		crawl_image( $elem->getAttribute('href') );
	}
}

sub crawl_page {
	my ($url) = @_;

	my $html = XML::LibXML->load_html(
		string            => $ua->get($url)->content,
		recover           => 2,
		suppress_errors   => 1,
		suppress_warnings => 1,
	);

	for my $elem ( $html->findnodes($xp_post) ) {
		my ($n_multi) = $elem->findnodes($xp_multi);
		my ($n_media) = $elem->findnodes($xp_media);

		$text_cache = $elem->textContent();

		if ($n_multi) {
			crawl_subpage( $n_multi->getAttribute('src') );
		}
		elsif ($n_media) {
			crawl_image( $n_media->getAttribute('href') );
		}
		else {
		}
	}
	for my $elem ( $html->findnodes($xp_next) ) {
		if (   ( $elem->textContent() =~ m{Next page|Older|Previous |Wander back} )
			or ( ( $elem->getAttribute('id') // '' ) eq 'nav-next' ) )
		{
			say "\tnext page: " . $elem->getAttribute('href');
			return ( $base_url . $elem->getAttribute('href') );
		}
	}
	return;
}

# recursive approach didn't work, caused by LibXML?
while ($url) {
	$url = crawl_page($url);
}
